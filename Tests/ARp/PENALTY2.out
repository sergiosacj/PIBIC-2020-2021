‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾ superscription ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
iter....: current iteration
f(x*)...: objective function avaluated at x(iter)
‖∇f(x)‖.: gradient norm used to calculate x(iter), so x in ‖∇f(x)‖ is equal to x(iter-1)
alpha...: step calculated by backtrack line search
‖d‖.....: search direction norm
‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
__________________________________________________________________________________________
iter   f(x*)            ‖∇f(x)‖         sigma          
‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
1      4.711629e+13     1.415132e-09    0.000000e+00   
2      4.711628e+13     5.177973e-02    0.000000e+00   
3      4.711628e+13     1.373274e+00    0.000000e+00   
4      4.711628e+13     4.285666e-03    0.000000e+00   
5      4.711628e+13     3.822792e-01    0.000000e+00   
6      4.711628e+13     6.223965e-11    0.000000e+00   
7      4.711628e+13     7.268660e-11    0.000000e+00   
8      4.711628e+13     3.358258e-01    0.000000e+00   
9      4.711628e+13     9.083538e-10    0.000000e+00   
10     4.711628e+13     1.995179e-10    0.000000e+00   
11     4.711628e+13     1.467883e-11    0.000000e+00   


‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾ summary statistics ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
Total iterations......: 12
AF....................: 22
AG....................: 23
AH....................: 11
f(x*).................: 0.000000e+00
‖∇f(x)‖...............: 0.000000e+00
sigma.................: 0.000000e+00
stop..................: Converged
‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾

